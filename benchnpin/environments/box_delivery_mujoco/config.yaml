boxes:
  num_boxes: 10
  clearance: 0.2

train:
  train_mode: false
  job_type: 'sam'
  job_name: 'sam_model'
  batch_size: 32
  checkpoint_freq: 6000
  exploration_timesteps: 6000
  final_exploration: 0.01
  gamma: 0.99
  grad_norm_clipping: 10
  job_id_to_resume: null
  learning_rate: 0.01
  learning_starts: 1000
  n_epochs: 10
  n_steps: 256
  replay_buffer_size: 10000
  resume_training: false
  target_update_freq: 1000
  total_timesteps: 60000
  use_correct_direction_reward: true
  verbose: 2
  weight_decay: 0.0001
  inactivity_cutoff: 1000
  tries_before_inactive: 3000

  
### ENVIRONMENT PARAMS
env:
  obstacle_config: small_columns # options are small_empty, small_columns, large_columns, large_divider
  room_length: 2.845
  room_width_small: 1.575
  room_width_large: 2.5
  shortest_path_channel_scale: 0.25
  local_map_pixel_width: 500
  local_map_pixel_width_sam: 96
  wall_thickness: 8.0 # Really high wall thickness to ensure nothing in vicinity of local maps
  invert_receptacle_map: false
  receptacle_position: [-0.6375, 1.2725]
  receptacle_size: 0.15

small_pillars:
  num_pillars: 6
  adjust_no_pillars: False # if true, number of pillars would be randomized after each reset to be between 1 and num_pillars
  pillar_half: [0.02, 0.02, 0.20]

large_pillars:
  num_pillars: 4
  adjust_no_pillars: True
  pillar_half: [0.16, 0.16, 0.10]

# NOT WORKING YET
large_divider:
  divider_half: 0
  divider_height: 0

agent:
  action_type: 'position' # options are velocity, heading, position
  length: 0.08 
  width: 0.09
  robot_clear: 0.15

rewards_sam:
  partial_rewards_scale: 0.2
  goal_reward: 1.0
  collision_penalty: 0.25
  non_movement_penalty: 0.25
  correct_direction_reward_scale: 1